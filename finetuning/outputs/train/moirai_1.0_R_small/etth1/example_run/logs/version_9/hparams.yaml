beta1: 0.9
beta2: 0.98
log_on_step: false
loss_func: !!python/object:uni2ts.loss.packed.distribution.PackedNLLLoss {}
lr: 0.001
max_dim: 128
max_mask_ratio: 0.5
min_mask_ratio: 0.15
min_patches: 2
module_kwargs:
  attn_dropout_p: 0.0
  d_model: 384
  distr_output: !!python/object:uni2ts.distribution.mixture.MixtureOutput
    components:
    - !!python/object:uni2ts.distribution.student_t.StudentTOutput {}
    - !!python/object:uni2ts.distribution.normal.NormalFixedScaleOutput
      scale: 0.001
    - !!python/object:uni2ts.distribution.negative_binomial.NegativeBinomialOutput {}
    - !!python/object:uni2ts.distribution.log_normal.LogNormalOutput {}
  dropout_p: 0.0
  max_seq_len: 512
  num_layers: 6
  patch_sizes: !!python/tuple
  - 8
  - 16
  - 32
  - 64
  - 128
  scaling: true
num_samples: 100
num_training_steps: 10000
num_warmup_steps: 0
val_metric:
- !!python/object:uni2ts.loss.packed.point.PackedMSELoss
  correction: 1
  epsilon: 1.0e-05
  normalize: !!python/object/apply:uni2ts.loss.packed.normalized.PointNormType
  - none
- !!python/object:uni2ts.loss.packed.normalized.PackedNRMSELoss
  correction: 1
  epsilon: 1.0e-05
  normalize: !!python/object/apply:uni2ts.loss.packed.normalized.PointNormType
  - absolute_target_squared
weight_decay: 0.1
